{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = input(\"Enter the path to the dataset CSV file: \")\n",
    "concrete = pd.read_csv(dataset)\n",
    "dependent = input(\"Enter the name of the dependent column: \")\n",
    "concrete_temp = concrete.copy()\n",
    "X = concrete_temp.drop(dependent, axis=1).copy()\n",
    "# Now, try accessing the column again\n",
    "y = concrete[dependent]\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "frequent_itemset_file_path = input(\"Enter the path to the frequent itemset CSV file: \")\n",
    "# Define your column names\n",
    "column_names = ['Frequent_Itemsets_CHARM']\n",
    "# Read the CSV file into a DataFrame with specified column names\n",
    "freq_item = pd.read_csv(frequent_itemset_file_path, names=column_names)\n",
    "one_hot_encoded = pd.get_dummies(freq_item['Frequent_Itemsets_CHARM'].str.split(expand=True), prefix='', prefix_sep='')\n",
    "from sklearn.impute import SimpleImputer\n",
    "def handle_missing_values(dataset):\n",
    "    # Check if there are missing values in the dataset\n",
    "    if dataset.isnull().any().any():\n",
    "        # Identify numerical and categorical columns\n",
    "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "        # Impute numerical columns with mean\n",
    "        imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
    "\n",
    "        # Impute categorical columns with the most frequent value (mode)\n",
    "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
    "\n",
    "        return dataset\n",
    "    else:\n",
    "        # If no missing values, return the original dataset\n",
    "        return dataset\n",
    "X = handle_missing_values(X)\n",
    "def binarize_dataframe(input_df, num_bins):\n",
    "    df = pd.DataFrame(input_df)\n",
    "    df_binarized = pd.DataFrame()\n",
    "\n",
    "    for column in df.columns:\n",
    "        column_name = column+'_binarized'\n",
    "        bins = pd.qcut(df[column], q=num_bins, labels=False, duplicates='drop')\n",
    "        df_binarized[column_name] = (bins == bins.max()).astype(int)\n",
    "\n",
    "    return df_binarized\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_intervals_df(data_frame, column_name, num_intervals):\n",
    "    # Extract the specified column\n",
    "    value_ag = data_frame[column_name]\n",
    "\n",
    "    #print(\"Extracted column values:\")\n",
    "    #print(value_ag)\n",
    "\n",
    "    # Check if the column contains numeric data\n",
    "    if not pd.api.types.is_numeric_dtype(value_ag):\n",
    "        raise ValueError(f\"The '{column_name}' column must contain numeric data.\")\n",
    "\n",
    "    # Check for missing values in the column\n",
    "    if value_ag.isnull().any():\n",
    "        raise ValueError(f\"The '{column_name}' column contains missing values. Please handle them before processing.\")\n",
    "\n",
    "    # Check if scaling and clipping is necessary\n",
    "    if value_ag.min() < 0 or value_ag.max() > 1:\n",
    "        # Rescale to the range [0, 1]\n",
    "        value_ag_scaled = (value_ag - value_ag.min()) / (value_ag.max() - value_ag.min())\n",
    "\n",
    "        # Clip values to the range [0, 1]\n",
    "        value_ag_scaled_clipped = np.clip(value_ag_scaled, 0, 1)\n",
    "    else:\n",
    "        # No need to rescale or clip\n",
    "        value_ag_scaled_clipped = value_ag\n",
    "\n",
    "    #print(\"Scaled and clipped column values:\")\n",
    "    #print(value_ag_scaled_clipped)\n",
    "\n",
    "    # Calculate bin edges dynamically within the [0, 1] range\n",
    "    bin_edges = np.linspace(0, 1, num=num_intervals + 1)\n",
    "\n",
    "    # Create intervals and assign labels for clipped values\n",
    "    data_inter_clipped = pd.cut(value_ag_scaled_clipped.values, bins=bin_edges, labels=[chr(ord('A') + i) for i in range(len(bin_edges) - 1)])\n",
    "\n",
    "    #print(\"Computed intervals:\")\n",
    "    #print(data_inter_clipped)\n",
    "\n",
    "    # Store intervals in a DataFrame\n",
    "    intervals_df = pd.DataFrame({\n",
    "        'Category': data_inter_clipped.categories,\n",
    "        'Min_Value': [value_ag_scaled_clipped[data_inter_clipped == category].min() for category in data_inter_clipped.categories],\n",
    "        'Max_Value': [value_ag_scaled_clipped[data_inter_clipped == category].max() for category in data_inter_clipped.categories]\n",
    "    })\n",
    "\n",
    "    return intervals_df\n",
    "num_intervals = 10\n",
    "dependent_column_name = dependent\n",
    "intervals_df = create_intervals_df(concrete, dependent_column_name, num_intervals)\n",
    "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
    "    # Fill NaN values with the mean of the column\n",
    "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
    "\n",
    "    # Extract the specified dependent column\n",
    "    value_ag_filled = dataset_filled[dependent_column_name]\n",
    "\n",
    "    # Define the target range (0-1)\n",
    "    target_range = (0, 1)\n",
    "\n",
    "    # Scale to the target range [0, 1] using np.interp\n",
    "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), target_range)\n",
    "\n",
    "    # Create intervals and assign labels for scaled values\n",
    "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
    "\n",
    "    # Store intervals in a DataFrame\n",
    "    result_df = pd.DataFrame({\n",
    "        'Category': data_inter_filled,\n",
    "        dependent_column_name: value_ag_scaled_filled\n",
    "    })\n",
    "\n",
    "    return result_df\n",
    "num_intervals = 10\n",
    "result_df = categorize_column(concrete, dependent, num_intervals)\n",
    "#Data Binary\n",
    "df = pd.DataFrame(concrete)\n",
    "num_bins = 15\n",
    "# Use the previously defined binarize_dataframe function\n",
    "df_binarized = binarize_dataframe(df, num_bins)\n",
    "dependent_column_name = dependent\n",
    "# Rename columns and create 'Data_binary'\n",
    "df_binarized.columns = df_binarized.columns.str.replace('_binarized', '')\n",
    "data_incidence_input = df_binarized.drop(columns=[dependent_column_name])\n",
    "Data_binary = data_incidence_input.copy()\n",
    "Data_binary['Category'] = result_df['Category']\n",
    "Data_binary[dependent_column_name] = df_binarized[dependent_column_name]\n",
    "#Jaccard similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_jaccard_similarity(Data_binary, one_hot_encoded):\n",
    "    # Number of rows in the Data_binary DataFrame\n",
    "    num_rows_binary = Data_binary.shape[0]\n",
    "\n",
    "    # Number of rows in the one_hot_encoded DataFrame\n",
    "    num_rows_encoded = one_hot_encoded.shape[0]\n",
    "\n",
    "    # Number of columns in the one_hot_encoded DataFrame\n",
    "    num_cols_encoded = one_hot_encoded.shape[1]\n",
    "\n",
    "    # Creating a matrix to store Jaccard similarities\n",
    "    result_matrix = np.zeros((num_rows_binary, num_rows_encoded))\n",
    "\n",
    "    # Calculating Jaccard similarity for each cell (pair of row in Data_binary and row in one_hot_encoded)\n",
    "    for i in range(num_rows_binary):\n",
    "        for j in range(num_rows_encoded):\n",
    "            set_row_binary = set(Data_binary.iloc[i, 1:])  # Exclude the 'Category' column\n",
    "            set_row_encoded = set(one_hot_encoded.iloc[j, :])\n",
    "            intersection_size = len(set_row_binary.intersection(set_row_encoded))\n",
    "            union_size = len(set_row_binary.union(set_row_encoded))\n",
    "            similarity = intersection_size / union_size if union_size != 0 else 0\n",
    "            result_matrix[i, j] = similarity\n",
    "\n",
    "    # Convert the result matrix to a DataFrame\n",
    "    result_df = pd.DataFrame(result_matrix, columns=one_hot_encoded.index)\n",
    "\n",
    "    return result_df\n",
    "jaccard_result = calculate_jaccard_similarity(Data_binary, one_hot_encoded)\n",
    "#Transform Matrix\n",
    "def transform_matrix(matrix):\n",
    "    # Create a copy of the input matrix\n",
    "    transformed_matrix = matrix.copy()\n",
    "\n",
    "    # Apply the transformation\n",
    "    transformed_matrix[transformed_matrix > 0.50] = 1\n",
    "    transformed_matrix[transformed_matrix <= 0.50] = 0\n",
    "    return transformed_matrix\n",
    "transformed_matrix = transform_matrix(jaccard_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first we try to add box_type inequalities\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.optimize import minimize\n",
    "# from scipy.optimize import Bounds\n",
    "# SX = csr_matrix(transformed_matrix)\n",
    "# def objective_function(weights, SX, OS):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     loss=-np.sum(OS*np.log(P+1e-10))\n",
    "#     return loss\n",
    "\n",
    "# def gradient_function(weights,SX,OS):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     gradients=SX.T@(P - OS)\n",
    "#     return gradients\n",
    "# #applying box_type inequality constraints\n",
    "# #we determine A and B using single width estimation\n",
    "# features=SX.shape[1]\n",
    "# sample=SX.shape[0]\n",
    "# #we chose W as 1\n",
    "# A=np.full(features,1/sample)\n",
    "# B=np.full(features,1/sample)\n",
    "# # calculating A,B using bayesian method\n",
    "# def optimize_weights(SX,OS,A,B):\n",
    "#     initial_weights=np.random.normal(0, 0.01, size=SX.shape[1])\n",
    "#     bounds=Bounds(-B,A)\n",
    "#     result=minimize(\n",
    "#         fun=objective_function,\n",
    "#         x0=initial_weights,\n",
    "#         args=(SX,OS),\n",
    "#         method='L-BFGS-B',\n",
    "#         jac=gradient_function,\n",
    "#         bounds=bounds\n",
    "#     )\n",
    "#     return result.x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for 1-norm penalozation\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "SX = csr_matrix(transformed_matrix)\n",
    "nfeatures=SX.shape[1]\n",
    "gamma=np.random.uniform(0, 0.1,nfeatures)\n",
    "delta=np.random.uniform(0, 0.1,nfeatures)\n",
    "C1=1\n",
    "def objective_function(weights, SX, OS,gamma,delta,C1):\n",
    "    P=np.exp(SX.dot(weights))\n",
    "    P/=np.sum(P)\n",
    "    loss=-np.sum(OS*np.log(P+1e-10))\n",
    "    extra=-np.sum(gamma**2+delta**2)/(2*C1)\n",
    "    return loss+extra\n",
    "# def objective_function(weights, SX, OS,gamma,delta,C1):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     loss=-np.sum(OS*np.log(P+1e-10))\n",
    "#     extra=-np.sum(np.abs(gamma)+np.abs(delta))/(2*C1)\n",
    "#     return loss+extra\n",
    "def gradient_function(weights,SX,OS,gamma,delta,C1):\n",
    "    P=np.exp(SX.dot(weights))\n",
    "    P/=np.sum(P)\n",
    "    gradients=SX.T@(P-OS)\n",
    "    return gradients\n",
    "#applying box_type inequality constraints\n",
    "#we determine A and B using single width estimation\n",
    "features=SX.shape[1]\n",
    "sample=SX.shape[0]\n",
    "#we chose W as 1\n",
    "A=np.full(features,1/sample)\n",
    "B=np.full(features,1/sample)\n",
    "# calculating A,B using bayesian method\n",
    "def optimize_weights(SX,OS,A,B,gamma,delta,C1):\n",
    "    initial_weights=np.random.normal(0,0.01,size=SX.shape[1])\n",
    "    bounds=Bounds(-B-gamma,A+delta)\n",
    "    result=minimize(\n",
    "        fun=objective_function,\n",
    "        x0=initial_weights,\n",
    "        args=(SX, OS, gamma, delta, C1),\n",
    "        method='L-BFGS-B',\n",
    "        jac=gradient_function,\n",
    "        bounds=bounds\n",
    "    )\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.optimize import minimize\n",
    "# SX = csr_matrix(transformed_matrix)\n",
    "# def objective_function(weights, SX, OS):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     loss=-np.sum(OS*np.log(P+1e-10))\n",
    "#     return loss\n",
    "\n",
    "# def gradient_function(weights,SX,OS):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     gradients=SX.T@(P - OS)\n",
    "#     return gradients\n",
    "# #applying box_type inequality constraints\n",
    "\n",
    "# def optimize_weights(SX,OS):\n",
    "#     initial_weights=np.random.normal(0, 0.01, size=SX.shape[1])\n",
    "#     result=minimize(\n",
    "#         fun=objective_function,\n",
    "#         x0=initial_weights,\n",
    "#         args=(SX,OS),\n",
    "#         method='L-BFGS-B',\n",
    "#         jac=gradient_function\n",
    "#     )\n",
    "#     return result.x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using gaussian MAP estimation\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.optimize import minimize\n",
    "# from scipy.optimize import Bounds\n",
    "# SX = csr_matrix(transformed_matrix)\n",
    "# nfeatures=SX.shape[1]\n",
    "# # we chose sigma as 1\n",
    "# def objective_function(weights, SX, OS,sigma):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     loss=-np.sum(OS*np.log(P+1e-10))\n",
    "#     regularization=-np.sum(np.abs(weights))/sigma\n",
    "#     return loss+regularization\n",
    "# def gradient_function(weights,SX,OS,sigma):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     gradients=SX.T@(P-OS)\n",
    "#     return gradients\n",
    "# features=SX.shape[1]\n",
    "# sample=SX.shape[0]\n",
    "\n",
    "# def optimize_weights(SX,OS,sigma):\n",
    "#     initial_weights=np.random.normal(0,0.01,size=SX.shape[1])\n",
    "#     result=minimize(\n",
    "#         fun=objective_function,\n",
    "#         x0=initial_weights,\n",
    "#         args=(SX, OS,sigma),\n",
    "#         method='L-BFGS-B',\n",
    "#         jac=gradient_function\n",
    "#     )\n",
    "#     return result.x\n",
    "# sigma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using bayesian method\n",
    "# #adding 1-norm penalization\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy.optimize import minimize, Bounds\n",
    "# import numpy as np\n",
    "\n",
    "# SX=csr_matrix(transformed_matrix)\n",
    "# nfeatures=SX.shape[1]\n",
    "# gamma=np.random.uniform(0, 0.1,nfeatures)\n",
    "# delta=np.random.uniform(0, 0.1,nfeatures)\n",
    "# C1=1\n",
    "# # Define objective function\n",
    "# def objective_function(weights, SX, OS,gamma,delta,C1):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     loss=-np.sum(OS*np.log(P+1e-10))\n",
    "#     # extra=-np.sum(gamma**2+delta**2)/(2*C1)\n",
    "#     extra=-np.sum(np.abs(gamma)+np.abs(delta))/(2*C1)\n",
    "#     return loss+extra\n",
    "\n",
    "# # Define gradient of the objective function\n",
    "# def gradient_function(weights, SX, OS,gamma,delta,C1):\n",
    "#     P=np.exp(SX.dot(weights))\n",
    "#     P/=np.sum(P)\n",
    "#     gradients=SX.T@(P-OS)\n",
    "#     return gradients\n",
    "\n",
    "# W=1\n",
    "# feature_counts=np.array(SX.sum(axis=0)).flatten()  \n",
    "# L=SX.shape[0]  \n",
    "# variances=(feature_counts*(1-feature_counts/L))/L\n",
    "\n",
    "# #Calculate A and B\n",
    "# A=W*np.sqrt(variances)\n",
    "# B=W*np.sqrt(variances)\n",
    "\n",
    "# def optimize_weights(SX, OS, A, B,gamma,delta,C1):\n",
    "#     initial_weights = np.random.normal(0, 0.01, size=SX.shape[1])\n",
    "#     bounds=Bounds(-B-gamma, A+delta)\n",
    "#     result=minimize(\n",
    "#         fun=objective_function,\n",
    "#         x0=initial_weights,\n",
    "#         args=(SX, OS,gamma,delta,C1),\n",
    "#         method='L-BFGS-B',\n",
    "#         jac=gradient_function,\n",
    "#         bounds=bounds\n",
    "#     )\n",
    "#     return result.x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.8831092375943906\n"
     ]
    }
   ],
   "source": [
    "def calculate_weights_for_category(category, Data_binary, one_hot_encoded,A,B,gamma,delta,C1):\n",
    "    jaccard_similarity_matrix=calculate_jaccard_similarity(Data_binary, one_hot_encoded)\n",
    "    transformed_matrix = transform_matrix(jaccard_similarity_matrix)\n",
    "    SXmat = csr_matrix(transformed_matrix)\n",
    "\n",
    "    # Optimize weights using LMVM\n",
    "    weights = optimize_weights(SXmat, np.ones(SXmat.shape[0]) / SXmat.shape[0],A,B,gamma,delta,C1)\n",
    "    return weights\n",
    "categories_weights = {}\n",
    "categories=Data_binary['Category'].unique()\n",
    "for category in categories:\n",
    "    weights = calculate_weights_for_category(category, Data_binary, one_hot_encoded,A,B,gamma,delta,C1)\n",
    "    categories_weights[category] = {'weights': weights}\n",
    "# Function to calculate P(x/c) after weights are optimized\n",
    "def calculate_x_by_c(X, one_hot_encoded, category_weights,A,B,gamma,delta,C1):\n",
    "    jaccard_result = calculate_jaccard_similarity(X, one_hot_encoded)\n",
    "    x_by_c = pd.DataFrame(index=X.index)\n",
    "\n",
    "    # Resetting index for consistency\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    jaccard_result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    for index, row in X.iterrows():\n",
    "        try:\n",
    "            category = row['Category']\n",
    "            jaccard_row = jaccard_result.iloc[index].values.reshape(1, -1)\n",
    "\n",
    "            category_info = category_weights.get(category)\n",
    "            if category_info is None:\n",
    "                raise ValueError(f\"No weights found for category: {category}\")\n",
    "            weights = category_info['weights']\n",
    "            weights = np.expand_dims(weights, axis=0)\n",
    "\n",
    "            # Perform matrix multiplication directly with weights\n",
    "            row_value = np.dot(jaccard_row, weights.T).squeeze()\n",
    "\n",
    "            # Store result\n",
    "            x_by_c.loc[index, 'P(x/c)'] = row_value\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in row {index}: {e}\")\n",
    "            raise\n",
    "\n",
    "    return x_by_c\n",
    "x_by_c_values = calculate_x_by_c(Data_binary, one_hot_encoded, categories_weights,A,B,gamma,delta,C1)\n",
    "\n",
    "# Calculate category probabilities\n",
    "def calculate_category_probabilities(data_frame, category_column):\n",
    "    category_counts = data_frame[category_column].value_counts()\n",
    "    category_probabilities = category_counts / len(data_frame)\n",
    "\n",
    "    return category_counts, pd.DataFrame({\n",
    "        category_column: category_probabilities.index,\n",
    "        'Probability': category_probabilities.values\n",
    "    })\n",
    "category_counts, category_probabilities_df= calculate_category_probabilities(Data_binary, 'Category')\n",
    "# Calculate maximum posterior category\n",
    "categories = np.sort(Data_binary['Category'].unique())\n",
    "\n",
    "# Set constant values\n",
    "record_prob = 1 / len(Data_binary)\n",
    "\n",
    "# Calculate record_prob / P(c) for each category\n",
    "record_prob_per_category = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'Record_Prob_Per_Category': record_prob / category_probabilities_df.loc[category_probabilities_df['Category'].isin(categories), 'Probability'].values\n",
    "})\n",
    "def calculate_max_posterior_category(binary_dataset, x_by_c_values, record_prob_per_category):\n",
    "    predicted_categories = []\n",
    "    for index, row_values in x_by_c_values.iterrows():\n",
    "        p_x_c = row_values['P(x/c)']\n",
    "        category_probs = p_x_c * record_prob_per_category['Record_Prob_Per_Category']\n",
    "        predicted_category = record_prob_per_category['Category'][category_probs.idxmax()]\n",
    "        predicted_categories.append(predicted_category)\n",
    "\n",
    "    binary_dataset['Predicted_Category'] = predicted_categories\n",
    "    return binary_dataset\n",
    "\n",
    "# Calculate RMSE for the predictions\n",
    "\n",
    "result = calculate_max_posterior_category(Data_binary, x_by_c_values, record_prob_per_category)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data_binary, Data_binary[dependent], test_size=0.3, random_state=42)\n",
    "def calculate_rmse(X_test, intervals_df):\n",
    "    prediction = calculate_max_posterior_category(Data_binary, x_by_c_values, record_prob_per_category)\n",
    "    predicted_column = prediction['Predicted_Category']\n",
    "\n",
    "    sum_squared_error = 0.0\n",
    "    for i, row in X_test.iterrows():\n",
    "        actual_category = row['Category']\n",
    "        predicted_category = predicted_column[i]\n",
    "\n",
    "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values[0]\n",
    "        predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values[0]\n",
    "\n",
    "        squared_difference = (predicted_max_value - actual_max_value) ** 2\n",
    "        sum_squared_error += squared_difference\n",
    "\n",
    "    mean_squared_error = sum_squared_error / len(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "result_rmse = calculate_rmse(X_test, intervals_df)\n",
    "print(\"Root Mean Squared Error (RMSE):\", result_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applying inequality constraints the rmse value decreased \n",
    "on applying l2 norm and l1 norm the rmse slightly decreased"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
