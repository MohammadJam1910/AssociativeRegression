{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in concrete DataFrame: Index(['RUN', 'SPEED1', 'TOTAL', 'SPEED2', 'NUMBER2', 'SENS', 'TIME',\n",
      "       'T20BOLT'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m     categories_weights[category] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m: weights}\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Calculate P(x/c) for the test set\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m x_by_c_values \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_x_by_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Calculate category probabilities and record probabilities\u001b[39;00m\n\u001b[0;32m    220\u001b[0m category_counts, category_probabilities_df \u001b[38;5;241m=\u001b[39m calculate_category_probabilities(X_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 119\u001b[0m, in \u001b[0;36mcalculate_x_by_c\u001b[1;34m(X, one_hot_encoded, category_weights)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     category \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 119\u001b[0m     jaccard_row \u001b[38;5;241m=\u001b[39m \u001b[43mjaccard_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m     category_info \u001b[38;5;241m=\u001b[39m category_weights\u001b[38;5;241m.\u001b[39mget(category)\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m category_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to handle missing values\n",
    "def handle_missing_values(dataset):\n",
    "    if dataset.isnull().any().any():\n",
    "        numerical_cols = dataset.select_dtypes(include=['number']).columns\n",
    "        categorical_cols = dataset.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "        imputer_numeric = SimpleImputer(strategy='mean')\n",
    "        dataset[numerical_cols] = imputer_numeric.fit_transform(dataset[numerical_cols])\n",
    "\n",
    "        imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "        dataset[categorical_cols] = imputer_categorical.fit_transform(dataset[categorical_cols])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Function to categorize the dependent variable\n",
    "def categorize_column(dataset, dependent_column_name, num_intervals):\n",
    "    # Ensure the dependent column exists in the dataset\n",
    "    if dependent_column_name not in dataset.columns:\n",
    "        raise ValueError(f\"Dependent column '{dependent_column_name}' does not exist in the dataset.\")\n",
    "\n",
    "    # Fill NaN values with the mean of the column\n",
    "    dataset_filled = dataset.fillna(dataset[dependent_column_name].mean())\n",
    "\n",
    "    # Extract the specified dependent column\n",
    "    value_ag_filled = dataset_filled[dependent_column_name]\n",
    "\n",
    "    # Check if the column contains numeric data\n",
    "    if not pd.api.types.is_numeric_dtype(value_ag_filled):\n",
    "        raise ValueError(f\"The '{dependent_column_name}' column must contain numeric data.\")\n",
    "\n",
    "    # Scale to the target range [0, 1] using np.interp\n",
    "    value_ag_scaled_filled = np.interp(value_ag_filled, (value_ag_filled.min(), value_ag_filled.max()), (0, 1))\n",
    "\n",
    "    # Create intervals and assign labels for scaled values\n",
    "    data_inter_filled = pd.cut(value_ag_scaled_filled, bins=num_intervals, labels=[chr(ord('A') + i) for i in range(num_intervals)])\n",
    "\n",
    "    # Store intervals in the dataset\n",
    "    dataset['Category'] = data_inter_filled\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Function to calculate Jaccard similarity\n",
    "def calculate_jaccard_similarity(Data_binary, one_hot_encoded):\n",
    "    num_rows_binary = Data_binary.shape[0]\n",
    "    num_rows_encoded = one_hot_encoded.shape[0]\n",
    "    result_matrix = np.zeros((num_rows_binary, num_rows_encoded))\n",
    "\n",
    "    for i in range(num_rows_binary):\n",
    "        for j in range(num_rows_encoded):\n",
    "            set_row_binary = set(Data_binary.iloc[i, 1:])\n",
    "            set_row_encoded = set(one_hot_encoded.iloc[j, :])\n",
    "            intersection_size = len(set_row_binary.intersection(set_row_encoded))\n",
    "            union_size = len(set_row_binary.union(set_row_encoded))\n",
    "            similarity = intersection_size / union_size if union_size != 0 else 0\n",
    "            result_matrix[i, j] = similarity\n",
    "\n",
    "    return pd.DataFrame(result_matrix, columns=one_hot_encoded.index)\n",
    "\n",
    "# Transform matrix for Jaccard similarity\n",
    "def transform_matrix(matrix):\n",
    "    transformed_matrix = matrix.copy()\n",
    "    transformed_matrix[transformed_matrix > 0.50] = 1\n",
    "    transformed_matrix[transformed_matrix <= 0.50] = 0\n",
    "    return transformed_matrix\n",
    "\n",
    "# Objective function for LMVM\n",
    "def objective_function(weights, SX, OS):\n",
    "    P = np.exp(SX.dot(weights))\n",
    "    P /= np.sum(P)\n",
    "    loss = -np.sum(OS * np.log(P + 1e-10))\n",
    "    return loss\n",
    "\n",
    "# Gradient function for LMVM\n",
    "def gradient_function(weights, SX, OS):\n",
    "    P = np.exp(SX.dot(weights))\n",
    "    P /= np.sum(P)\n",
    "    gradients = SX.T @ (P - OS)\n",
    "    return gradients\n",
    "\n",
    "# Optimize weights using LMVM\n",
    "def optimize_weights(SX, OS):\n",
    "    initial_weights = np.random.normal(0, 0.01, size=SX.shape[1])\n",
    "    result = minimize(\n",
    "        fun=objective_function,\n",
    "        x0=initial_weights,\n",
    "        args=(SX, OS),\n",
    "        method='L-BFGS-B',\n",
    "        jac=gradient_function\n",
    "    )\n",
    "    return result.x\n",
    "\n",
    "# Function to calculate weights for each category (using LMVM)\n",
    "def calculate_weights_for_category(category, Data_binary, one_hot_encoded):\n",
    "    jaccard_similarity_matrix = calculate_jaccard_similarity(Data_binary, one_hot_encoded)\n",
    "    transformed_matrix = transform_matrix(jaccard_similarity_matrix)\n",
    "    SXmat = csr_matrix(transformed_matrix)\n",
    "\n",
    "    # Optimize weights using LMVM\n",
    "    weights = optimize_weights(SXmat, np.ones(SXmat.shape[0]) / SXmat.shape[0])\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Function to calculate P(x/c) after weights are optimized\n",
    "def calculate_x_by_c(X, one_hot_encoded, category_weights):\n",
    "    jaccard_result = calculate_jaccard_similarity(X, one_hot_encoded)\n",
    "    x_by_c = pd.DataFrame(index=X.index)\n",
    "\n",
    "    for index, row in X.iterrows():\n",
    "        try:\n",
    "            category = row['Category']\n",
    "            jaccard_row = jaccard_result.iloc[index].values.reshape(1, -1)\n",
    "\n",
    "            category_info = category_weights.get(category)\n",
    "            if category_info is None:\n",
    "                raise ValueError(f\"No weights found for category: {category}\")\n",
    "            weights = category_info['weights']\n",
    "            weights = np.expand_dims(weights, axis=0)\n",
    "\n",
    "            # Perform matrix multiplication directly with weights\n",
    "            row_value = np.dot(jaccard_row, weights.T).squeeze()\n",
    "\n",
    "            # Store result\n",
    "            x_by_c.loc[index, 'P(x/c)'] = row_value\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in row {index}: {e}\")\n",
    "            raise\n",
    "\n",
    "    return x_by_c\n",
    "\n",
    "# Calculate category probabilities\n",
    "def calculate_category_probabilities(data_frame, category_column):\n",
    "    category_counts = data_frame[category_column].value_counts()\n",
    "    category_probabilities = category_counts / len(data_frame)\n",
    "\n",
    "    return category_counts, pd.DataFrame({\n",
    "        category_column: category_probabilities.index,\n",
    "        'Probability': category_probabilities.values\n",
    "    })\n",
    "\n",
    "# Calculate maximum posterior category\n",
    "def calculate_max_posterior_category(binary_dataset, x_by_c_values, record_prob_per_category):\n",
    "    predicted_categories = []\n",
    "    for index, row_values in x_by_c_values.iterrows():\n",
    "        p_x_c = row_values['P(x/c)']\n",
    "        category_probs = p_x_c * record_prob_per_category['Record_Prob_Per_Category']\n",
    "        predicted_category = record_prob_per_category['Category'][category_probs.idxmax()]\n",
    "        predicted_categories.append(predicted_category)\n",
    "\n",
    "    binary_dataset['Predicted_Category'] = predicted_categories\n",
    "    return binary_dataset\n",
    "\n",
    "# Calculate RMSE for the predictions\n",
    "def calculate_rmse(X_test, intervals_df):\n",
    "    prediction = calculate_max_posterior_category(Data_binary, x_by_c_values, record_prob_per_category)\n",
    "    predicted_column = prediction['Predicted_Category']\n",
    "\n",
    "    sum_squared_error = 0.0\n",
    "    for i, row in X_test.iterrows():\n",
    "        actual_category = row['Category']\n",
    "        predicted_category = predicted_column[i]\n",
    "\n",
    "        actual_max_value = intervals_df[intervals_df['Category'] == actual_category]['Max_Value'].values[0]\n",
    "        predicted_max_value = intervals_df[intervals_df['Category'] == predicted_category]['Max_Value'].values[0]\n",
    "\n",
    "        squared_difference = (predicted_max_value - actual_max_value) ** 2\n",
    "        sum_squared_error += squared_difference\n",
    "\n",
    "    mean_squared_error = sum_squared_error / len(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse\n",
    "\n",
    "# Main execution\n",
    "dataset = input(\"Enter the path to the dataset CSV file: \")\n",
    "concrete = pd.read_csv(dataset)\n",
    "\n",
    "# Specify the dependent column correctly\n",
    "dependent = 'T20BOLT'\n",
    "\n",
    "# Check the structure of the DataFrame\n",
    "print(\"Columns in concrete DataFrame:\", concrete.columns)\n",
    "\n",
    "# Categorize the dependent variable\n",
    "num_intervals = 10\n",
    "concrete = categorize_column(concrete, dependent, num_intervals)\n",
    "\n",
    "# Prepare the data for training\n",
    "X = concrete.drop(dependent, axis=1)\n",
    "y = concrete[dependent]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Load one-hot encoded frequent itemsets\n",
    "frequent_itemset_file_path = input(\"Enter the path to the frequent itemset CSV file: \")\n",
    "column_names = ['Frequent_Itemsets_CHARM']\n",
    "freq_item = pd.read_csv(frequent_itemset_file_path, names=column_names)\n",
    "one_hot_encoded = pd.get_dummies(freq_item['Frequent_Itemsets_CHARM'].str.split(expand=True), prefix='', prefix_sep='')\n",
    "\n",
    "# Calculate weights for each category\n",
    "categories_weights = {}\n",
    "categories = X_train['Category'].unique()\n",
    "\n",
    "for category in categories:\n",
    "    weights = calculate_weights_for_category(category, X_train, one_hot_encoded)\n",
    "    categories_weights[category] = {'weights': weights}\n",
    "\n",
    "# Calculate P(x/c) for the test set\n",
    "x_by_c_values = calculate_x_by_c(X_test, one_hot_encoded, categories_weights)\n",
    "\n",
    "# Calculate category probabilities and record probabilities\n",
    "category_counts, category_probabilities_df = calculate_category_probabilities(X_train, 'Category')\n",
    "record_prob_per_category = pd.DataFrame({\n",
    "    'Category': np.sort(X_train['Category'].unique()),\n",
    "    'Record_Prob_Per_Category': 1 / len(X_train) / category_probabilities_df['Probability']\n",
    "})\n",
    "\n",
    "# Calculate RMSE\n",
    "result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
