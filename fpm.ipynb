{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:weka.core.jvm:Adding bundled jars\n",
      "DEBUG:weka.core.jvm:Classpath=['c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\weka\\\\lib\\\\arpack_combined.jar', 'c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\weka\\\\lib\\\\core.jar', 'c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\weka\\\\lib\\\\mtj.jar', 'c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\weka\\\\lib\\\\python-weka-wrapper.jar', 'c:\\\\Users\\\\hp\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\weka\\\\lib\\\\weka.jar']\n",
      "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
      "DEBUG:weka.core.jvm:Package support disabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during discretization: java.lang.Exception: Illegal options: -M \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import weka.core.jvm as jvm\n",
    "from weka.core.converters import Loader, Saver\n",
    "from weka.filters import Filter\n",
    "\n",
    "# Set JAVA_HOME environment variable (ensure this path is correct)\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-21\"\n",
    "\n",
    "# Start the JVM\n",
    "if not jvm.started:\n",
    "    jvm.start()\n",
    "\n",
    "try:\n",
    "    # Load the dataset in ARFF format\n",
    "    loader = Loader(classname=\"weka.core.converters.ArffLoader\")\n",
    "    data = loader.load_file(\"converted.arff\")\n",
    "    \n",
    "    # Ensure the class attribute is set correctly (MDLP requires supervised learning)\n",
    "    data.class_is_last()\n",
    "\n",
    "    # Apply MDLP-based discretization (information-theoretic measure)\n",
    "    discretize = Filter(classname=\"weka.filters.supervised.attribute.Discretize\")\n",
    "    discretize.options = [\"-M\", \"-R\", \"first-last\"]  # -M for MDLP, -R for all attributes\n",
    "\n",
    "    # Prepare the filter with the dataset structure\n",
    "    discretize.inputformat(data)\n",
    "    \n",
    "    # Apply the filter\n",
    "    discretized_data = discretize.filter(data)\n",
    "\n",
    "    # Save the discretized dataset as a new ARFF file\n",
    "    saver = Saver(classname=\"weka.core.converters.ArffSaver\")\n",
    "    saver.save_file(discretized_data, \"discretized_dataset_mdlp.arff\")\n",
    "\n",
    "    print(\"Discretization complete with minimal data loss using MDLP.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during discretization:\", e)\n",
    "\n",
    "finally:\n",
    "    # Stop the JVM\n",
    "    if jvm.started:\n",
    "        jvm.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'discretized_dataset.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01marff\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiscretized_dataset.arff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     arff_data\u001b[38;5;241m=\u001b[39marff\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m      5\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(arff_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m[attr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m arff_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributes\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'discretized_dataset.arff'"
     ]
    }
   ],
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "with open(\"discretized_dataset.arff\", \"r\") as file:\n",
    "    arff_data=arff.load(file)\n",
    "df=pd.DataFrame(arff_data['data'], columns=[attr[0] for attr in arff_data['attributes']])\n",
    "df.to_csv(\"discretized_dataset1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretization complete using information-theoretic measures.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(y):\n",
    "    counts = np.bincount(y)\n",
    "    probabilities = counts / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "# Function to calculate information gain\n",
    "def information_gain(y, y_left, y_right):\n",
    "    parent_entropy = entropy(y)\n",
    "    n = len(y)\n",
    "    n_left, n_right = len(y_left), len(y_right)\n",
    "    child_entropy = (n_left / n) * entropy(y_left) + (n_right / n) * entropy(y_right)\n",
    "    return parent_entropy - child_entropy\n",
    "\n",
    "# Function to find the best split point\n",
    "def best_split(X, y):\n",
    "    best_gain = -1\n",
    "    best_split = None\n",
    "    for split in np.unique(X):\n",
    "        y_left = y[X <= split]\n",
    "        y_right = y[X > split]\n",
    "        if len(y_left) > 0 and len(y_right) > 0:\n",
    "            gain = information_gain(y, y_left, y_right)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_split = split\n",
    "    return best_split\n",
    "\n",
    "# Recursive function to discretize based on information gain\n",
    "def discretize_column(X, y, min_gain=0.01):\n",
    "    split = best_split(X, y)\n",
    "    if split is None:\n",
    "        return [(-np.inf, np.inf)]\n",
    "    y_left = y[X <= split]\n",
    "    y_right = y[X > split]\n",
    "    gain = information_gain(y, y_left, y_right)\n",
    "    if gain < min_gain:\n",
    "        return [(-np.inf, np.inf)]\n",
    "    left_intervals = discretize_column(X[X <= split], y_left, min_gain)\n",
    "    right_intervals = discretize_column(X[X > split], y_right, min_gain)\n",
    "    return [(interval[0], split) for interval in left_intervals] + [(split, interval[1]) for interval in right_intervals]\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"concatenated_SWaT_Dataset.csv\")\n",
    "\n",
    "# Separate features and the target\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Discretize each column in X\n",
    "intervals = {}\n",
    "for column in X.columns:\n",
    "    intervals[column] = discretize_column(X[column].values, y.values)\n",
    "\n",
    "# Apply intervals to convert the continuous data to categorical data\n",
    "X_discretized = X.copy()\n",
    "for column, splits in intervals.items():\n",
    "    for i, (low, high) in enumerate(splits):\n",
    "        X_discretized[column] = np.where((X[column] > low) & (X[column] <= high), i, X_discretized[column])\n",
    "\n",
    "# Save discretized data\n",
    "discretized_data = pd.concat([X_discretized, y], axis=1)\n",
    "discretized_data.to_csv(\"discretized_dataset_entropy.csv\", index=False)\n",
    "\n",
    "print(\"Discretization complete using information-theoretic measures.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretized 'FIT101' into bins: [-inf, 0.0003202765074092895, 2.670627474784851, 2.696249485015869, 2.6967300176620483, inf]\n",
      "Discretized 'LIT101' into bins: [-inf, 812.6890258789062, 813.1798095703125, 814.3966064453125, 858.6737670898438, inf]\n",
      "Discretized 'MV101' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'P101' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P102' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'AIT201' into bins: [-inf, 188.26900482177734, 188.58944702148438, 192.78710174560547, 198.3946533203125, inf]\n",
      "Discretized 'AIT202' into bins: [-inf, 8.51964282989502, 8.534862518310547, 8.565303802490234, 8.62826919555664, inf]\n",
      "Discretized 'AIT203' into bins: [-inf, 339.15660095214844, 342.29685974121094, 353.1658477783203, 370.5075988769531, inf]\n",
      "Discretized 'FIT201' into bins: [-inf, 6.40758516965434e-05, 2.4187999963760376, 2.435523509979248, 2.7764716148376465, inf]\n",
      "Discretized 'MV201' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'P201' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P202' into bins: [-inf, inf]\n",
      "Discretized 'P203' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P204' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P205' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P206' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'DPIT301' into bins: [-inf, 1.9959665536880493, 2.0151734352111816, 2.0439844131469727, 33.04965019226074, inf]\n",
      "Discretized 'FIT301' into bins: [-inf, 0.00012811050692107528, 2.195496916770935, 2.2004929780960083, 2.2107419967651367, inf]\n",
      "Discretized 'LIT301' into bins: [-inf, 785.8912353515625, 1010.7380065917969, 1013.5019836425781, 1014.0030212402344, inf]\n",
      "Discretized 'MV301' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'MV302' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'MV303' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'MV304' into bins: [-inf, 0.5, 1.5, inf]\n",
      "Discretized 'P301' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P302' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'AIT401' into bins: [-inf, 148.76709747314453, 148.80560302734375, 148.8104019165039, 148.81520080566406, inf]\n",
      "Discretized 'AIT402' into bins: [-inf, 141.36119842529297, 154.53730010986328, 155.5883026123047, 235.86260223388672, inf]\n",
      "Discretized 'FIT401' into bins: [-inf, 0.7367619574069977, 1.279787003993988, 1.7080060243606567, 1.7253069877624512, inf]\n",
      "Discretized 'LIT401' into bins: [-inf, 245.37940216064453, 250.5126495361328, 775.4742736816406, 999.9872131347656, inf]\n",
      "Discretized 'P401' into bins: [-inf, inf]\n",
      "Discretized 'P402' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P403' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P404' into bins: [-inf, inf]\n",
      "Discretized 'UV401' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'AIT501' into bins: [-inf, 7.433703184127808, 7.701262474060059, 7.763746976852417, 7.926044940948486, inf]\n",
      "Discretized 'AIT502' into bins: [-inf, 144.80905151367188, 185.2729949951172, 216.7008514404297, 218.18765258789062, inf]\n",
      "Discretized 'AIT503' into bins: [-inf, 247.0039520263672, 260.2377624511719, 266.06959533691406, 270.33135986328125, inf]\n",
      "Discretized 'AIT504' into bins: [-inf, 14.938474655151367, 15.399894714355469, 15.745965003967285, 254.69815063476562, inf]\n",
      "Discretized 'FIT501' into bins: [-inf, 0.0014739810139872134, 1.2041784524917603, 1.633619487285614, 1.7165470123291016, inf]\n",
      "Discretized 'FIT502' into bins: [-inf, 0.0012168569955974817, 0.0013449469697661698, 0.0014730370021425188, 1.1537724733352661, inf]\n",
      "Discretized 'FIT503' into bins: [-inf, 0.0016003584605641663, 0.001728387491311878, 0.7229139506816864, 0.7386614978313446, inf]\n",
      "Discretized 'FIT504' into bins: [-inf, 3.2027670386014506e-05, 0.00012813300054403953, 0.2989782989025116, 0.3103161007165909, inf]\n",
      "Discretized 'P501' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P502' into bins: [-inf, inf]\n",
      "Discretized 'PIT501' into bins: [-inf, 9.516790390014648, 10.341899871826172, 163.36359405517578, 176.48519897460938, inf]\n",
      "Discretized 'PIT502' into bins: [-inf, 0.2723224461078644, 0.9691474437713623, 1.3936499953269958, 1.6579625010490417, inf]\n",
      "Discretized 'PIT503' into bins: [-inf, 3.284414529800415, 4.0454370975494385, 134.54080200195312, 144.6824493408203, inf]\n",
      "Discretized 'FIT601' into bins: [-inf, 3.203793676220812e-05, 9.612585199647583e-05, 0.00019222774426452816, 0.0003524171479512006, inf]\n",
      "Discretized 'P601' into bins: [-inf, inf]\n",
      "Discretized 'P602' into bins: [-inf, 1.5, inf]\n",
      "Discretized 'P603' into bins: [-inf, inf]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data = pd.read_csv(\"concatenated_SWaT_Dataset.csv\")\n",
    "target_column = 'Normal/Attack'\n",
    "\n",
    "def discretize_feature_using_tree(X, y, max_leaf_nodes=5):\n",
    "    # Train a decision tree classifier to find split points based on the target variable\n",
    "    clf = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X.reshape(-1, 1), y)\n",
    "    thresholds = clf.tree_.threshold[clf.tree_.threshold != -2]  # Get decision thresholds\n",
    "    bins = [-np.inf] + sorted(thresholds.tolist()) + [np.inf]    # Define bins using thresholds\n",
    "    return np.digitize(X, bins), bins                            # Digitize values into bins\n",
    "\n",
    "def discretize_dataset(data, target_column, max_leaf_nodes=5):\n",
    "    y = data[target_column].values  # Extract target variable\n",
    "    discretized_data = data.copy()  # Copy the original data to add discretized columns\n",
    "\n",
    "    for col in data.columns:\n",
    "        if col != target_column:  # Skip the target column\n",
    "            X = data[col].values\n",
    "            discretized_values, bins = discretize_feature_using_tree(X, y, max_leaf_nodes)\n",
    "            discretized_data[f'{col}_discretized'] = discretized_values\n",
    "            print(f\"Discretized '{col}' into bins: {bins}\")\n",
    "\n",
    "    return discretized_data\n",
    "\n",
    "\n",
    "target_column = 'Normal/Attack'  # Set this to the name of your target column\n",
    "discretized_data = discretize_dataset(data, target_column)\n",
    "discretized_data.to_csv(\"discretized_dataset_tree.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
